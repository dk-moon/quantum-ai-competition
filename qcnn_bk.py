import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torchvision import datasets, transforms

import pennylane as qml
from pennylane import numpy as np
from sklearn.preprocessing import normalize
from tqdm.auto import tqdm
import matplotlib.pyplot as plt

# -----------------------
# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# -----------------------
def load_fashion_mnist_binary():
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    train_set = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)
    test_set = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)

    def filter(dataset):
        indices = [i for i, (_, y) in enumerate(dataset) if y in [0, 6]]
        data = torch.stack([dataset[i][0] for i in indices])
        labels = torch.tensor([int(dataset[i][1] == 6) for i in indices])  # 0 â†’ 0, 6 â†’ 1
        return data, labels

    return filter(train_set), filter(test_set)

(train_x, train_y), (test_x, test_y) = load_fashion_mnist_binary()

# í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
print(f"Train data - Class 0: {(train_y == 0).sum()}, Class 1: {(train_y == 1).sum()}")
print(f"Test data - Class 0: {(test_y == 0).sum()}, Class 1: {(test_y == 1).sum()}")

# í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
print(f"Train data - Class 0: {(train_y == 0).sum()}, Class 1: {(train_y == 1).sum()}")
print(f"Test data - Class 0: {(test_y == 0).sum()}, Class 1: {(test_y == 1).sum()}")

# -----------------------
# QNN ì •ì˜ (8-qubit í’ë¶€í•œ Quantum Circuit)
# -----------------------
n_qubits = 8  # 8-qubit ì‹œìŠ¤í…œ
n_qnn_params = 24  # ë” ë§ì€ íŒŒë¼ë¯¸í„°ë¡œ í‘œí˜„ë ¥ í–¥ìƒ
dev = qml.device("default.qubit", wires=n_qubits)

# í’ë¶€í•œ 8-qubit Quantum Circuit ì •ì˜ (24 íŒŒë¼ë¯¸í„°)
@qml.qnode(dev, interface="torch")
def quantum_circuit(inputs, qnn_params):
    # Data encoding - 8ê°œ ì…ë ¥ì„ 8ê°œ qubitì— ê°ê° ì¸ì½”ë”©
    for i in range(n_qubits):
        qml.H(wires=i)  # ëª¨ë“  qubitì„ superposition ìƒíƒœë¡œ
        qml.RZ(2.*inputs[i], wires=i)  # ë°ì´í„° ì¸ì½”ë”©
    
    # Entangling layer 1 - ì¸ì ‘í•œ qubitë“¤ ê°„ì˜ entanglement
    for i in range(n_qubits - 1):
        qml.CNOT(wires=[i, i+1])
    
    # Variational layer 1 - ëª¨ë“  qubitì— RY íšŒì „ (8ê°œ íŒŒë¼ë¯¸í„°)
    for i in range(n_qubits):
        qml.RY(2.*qnn_params[i], wires=i)
    
    # Circular entanglement
    qml.CNOT(wires=[7, 0])  # ë§ˆì§€ë§‰ê³¼ ì²« ë²ˆì§¸ ì—°ê²°
    
    # Variational layer 2 - ëª¨ë“  qubitì— RX íšŒì „ (8ê°œ íŒŒë¼ë¯¸í„°)
    for i in range(n_qubits):
        qml.RX(2.*qnn_params[8 + i], wires=i)
    
    # ë” ë³µì¡í•œ entanglement íŒ¨í„´
    for i in range(0, n_qubits-1, 2):
        qml.CNOT(wires=[i, i+1])
    
    # ì¶”ê°€ entanglement - í™€ìˆ˜ ì¸ë±ìŠ¤ ê°„ ì—°ê²°
    for i in range(1, n_qubits-2, 2):
        qml.CNOT(wires=[i, i+2])
    
    # Variational layer 3 - ëª¨ë“  qubitì— RZ íšŒì „ (8ê°œ íŒŒë¼ë¯¸í„°)
    for i in range(n_qubits):
        qml.RZ(2.*qnn_params[16 + i], wires=i)
    
    return qml.expval(qml.PauliZ(0))

# QNN íŒŒë¼ë¯¸í„°ë¥¼ ì§ì ‘ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
class QuantumLayer(nn.Module):
    def __init__(self, n_params):
        super().__init__()
        self.qnn_params = nn.Parameter(torch.randn(n_params, dtype=torch.float64) * 0.1)
        
    def forward(self, x):
        # ë°°ì¹˜ ì²˜ë¦¬
        results = []
        for i in range(x.size(0)):
            # 8ì°¨ì› ì…ë ¥ ì‚¬ìš©
            input_data = x[i]  # ì´ë¯¸ 8ì°¨ì›ì´ì–´ì•¼ í•¨
            result = quantum_circuit(input_data, self.qnn_params)
            results.append(result)
        return torch.stack(results)

qlayer = QuantumLayer(n_qnn_params)

# -----------------------
# ì •í™•íˆ 45K íŒŒë¼ë¯¸í„° ëª¨ë¸ ì„¤ê³„
# -----------------------
# ìµœì¢… ì •í™•í•œ 45K ëª¨ë¸ (ë¯¸ì„¸ ì¡°ì •)
class QCNN_Model(nn.Module):
    def __init__(self):
        super().__init__()
        # ëª©í‘œ: ì •í™•íˆ 45,000 íŒŒë¼ë¯¸í„°
        # QNN: 24 íŒŒë¼ë¯¸í„° (8-qubit í’ë¶€í•œ circuit)
        # ë‚¨ì€ ì˜ˆì‚°: 44,976 íŒŒë¼ë¯¸í„°
        
        # 8ì°¨ì› ì¶œë ¥ CNN (8-qubit QNNì— ë§ì¶¤)
        self.cnn = nn.Sequential(
            # 28x28 â†’ 14x14
            nn.Conv2d(1, 2, 5, stride=2, padding=2),     # params: 1*2*25 + 2 = 52
            nn.ReLU(),
            
            # 14x14 â†’ 7x7
            nn.Conv2d(2, 2, 3, stride=2, padding=1),     # params: 2*2*9 + 2 = 38
            nn.ReLU(),
            
            # 7x7 â†’ 4x4
            nn.Conv2d(2, 2, 3, stride=1, padding=1),     # params: 2*2*9 + 2 = 38
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(2),                     # 2x2x2 = 8
            
            nn.Flatten(),
            nn.Linear(8, 8),                             # params: 8*8 + 8 = 72 (8-qubit ì…ë ¥)
            nn.ReLU(),
            nn.Dropout(0.1)
        ).double()
        
        self.qnn = qlayer
        
        # ì¶•ì†Œëœ ë¶„ë¥˜ê¸° (QNN íŒŒë¼ë¯¸í„° ì¦ê°€ë¡œ ì¸í•œ ì¡°ì •)
        self.classifier = nn.Sequential(
            nn.Linear(1, 200),      # params: 1*200 + 200 = 400
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(200, 100),    # params: 200*100 + 100 = 20,100
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(100, 50),     # params: 100*50 + 50 = 5,050
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(50, 20),      # params: 50*20 + 20 = 1,020
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(20, 1),       # params: 20*1 + 1 = 21
            # Sigmoid ì œê±° - BCEWithLogitsLoss ì‚¬ìš©
        ).double()
        
        # ì´ ê³„ì‚°: CNN(ì•½ 202) + QNN(24) + Classifier(ì•½ 26,591) = ì•½ 26,817
        
    def forward(self, x):
        features = self.cnn(x)  # ì´ì œ 8ì°¨ì› ì¶œë ¥ (8-qubit QNN ì…ë ¥ì— ë§ì¶¤)
        
        # QNN ì²˜ë¦¬
        q_out = self.qnn(features).unsqueeze(1)  # [batch_size, 1]
        
        return self.classifier(q_out)

# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° í•¨ìˆ˜
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

# -----------------------
# í•™ìŠµ ì„¤ì •
# -----------------------
# ë°ì´í„°ë¥¼ double precisionìœ¼ë¡œ ë³€í™˜
train_x = train_x.double()
test_x = test_x.double()
train_y = train_y.double().unsqueeze(1)
test_y = test_y.double().unsqueeze(1)

train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=32, shuffle=True)
test_loader = DataLoader(TensorDataset(test_x, test_y), batch_size=32)

epochs = 300

# ëª¨ë¸ ì„ íƒ ë° íŒŒë¼ë¯¸í„° í™•ì¸
model = QCNN_Model()
total_params = count_parameters(model)

print(f"Model parameters: {total_params:,}")
print(f"Train data shape: {train_x.shape}, Test data shape: {test_x.shape}")

# 50K ì œí•œ í™•ì¸
if total_params <= 50000:
    print(f"âœ… Parameter limit satisfied: {total_params:,} â‰¤ 50,000")
else:
    print(f"âŒ Parameter limit exceeded: {total_params:,} > 50,000")

# ê°œì„ ëœ í•™ìŠµë¥  ì„¤ì •
optimizer = optim.AdamW([
    {'params': model.cnn.parameters(), 'lr': 0.001, 'weight_decay': 1e-4},
    {'params': model.qnn.parameters(), 'lr': 0.005, 'weight_decay': 1e-5},  # QNN í•™ìŠµë¥  ì¡°ì •
    {'params': model.classifier.parameters(), 'lr': 0.002, 'weight_decay': 1e-4}
])

# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• í•´ê²°)
class_counts = torch.bincount(train_y.long().flatten())
class_weights = 1.0 / class_counts.float()
class_weights = class_weights / class_weights.sum() * 2  # ì •ê·œí™”

print(f"Class weights: {class_weights}")

# ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì†ì‹¤ í•¨ìˆ˜
criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1]/class_weights[0])
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.8, patience=30)

# -----------------------
# í›ˆë ¨ ë£¨í”„
# -----------------------
best_test_acc = 0
patience_counter = 0
patience = 25

train_losses, train_accs = [], []
test_losses, test_accs = [], []

print("Starting CNN-QNN training...")

for epoch in tqdm(range(epochs)):
    model.train()
    total_loss, total_acc = 0, 0
    
    for xb, yb in train_loader:
        optimizer.zero_grad()
        preds = model(xb)
        loss = criterion(preds, yb)
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        loss.backward()
        optimizer.step()
        
        acc = ((torch.sigmoid(preds) > 0.5).double() == yb).double().mean()
        total_loss += loss.item()
        total_acc += acc.item()

    # í‰ê°€
    model.eval()
    test_loss, test_acc = 0, 0
    with torch.no_grad():
        for xb, yb in test_loader:
            preds = model(xb)
            loss = criterion(preds, yb)
            acc = ((torch.sigmoid(preds) > 0.5).double() == yb).double().mean()
            test_loss += loss.item()
            test_acc += acc.item()
    
    avg_train_loss = total_loss / len(train_loader)
    avg_train_acc = total_acc / len(train_loader)
    avg_test_loss = test_loss / len(test_loader)
    avg_test_acc = test_acc / len(test_loader)
    
    # ê¸°ë¡
    train_losses.append(avg_train_loss)
    train_accs.append(avg_train_acc)
    test_losses.append(avg_test_loss)
    test_accs.append(avg_test_acc)
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
    scheduler.step(avg_test_acc)
    
    # ìµœê³  ì„±ëŠ¥ ì¶”ì 
    if avg_test_acc > best_test_acc:
        best_test_acc = avg_test_acc
        patience_counter = 0
        torch.save(model.state_dict(), 'best_cnn_qnn_45k.pth')
    else:
        patience_counter += 1
    
    if epoch % 15 == 0 or patience_counter == 0:
        print(f"[Epoch {epoch+1}] Train Acc: {avg_train_acc:.4f} | Test Acc: {avg_test_acc:.4f} | Best: {best_test_acc:.4f}")
    
    # ì¡°ê¸° ì¢…ë£Œ
    if patience_counter >= patience:
        print(f"Early stopping at epoch {epoch+1}")
        break

print(f"\nğŸ¯ Training complete! Best Test Accuracy: {best_test_acc:.4f}")

# ìƒì„¸í•œ ëª¨ë¸ ë¶„ì„
cnn_params = sum(p.numel() for p in model.cnn.parameters())
qnn_params = sum(p.numel() for p in model.qnn.parameters())
classifier_params = sum(p.numel() for p in model.classifier.parameters())

print(f"\nğŸ“Š Model Analysis:")
print(f"Total parameters: {total_params:,} / 50,000 ({total_params/50000*100:.1f}%)")
print(f"CNN parameters: {cnn_params:,} ({cnn_params/total_params*100:.1f}%)")
print(f"QNN parameters: {qnn_params:,} ({qnn_params/total_params*100:.1f}%)")
print(f"Classifier parameters: {classifier_params:,} ({classifier_params/total_params*100:.1f}%)")

print(f"\nâœ… Constraint Verification:")
print(f"Parameter limit (â‰¤50K): {total_params <= 50000} ({total_params:,})")
print(f"QNN parameters (8-60): {8 <= qnn_params <= 60} ({qnn_params})")
print(f"Hybrid model: âœ… (CNN + QNN + Classifier)")

# ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss', alpha=0.8)
plt.plot(test_losses, label='Test Loss', alpha=0.8)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Progress - Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(train_accs, label='Train Accuracy', alpha=0.8)
plt.plot(test_accs, label='Test Accuracy', alpha=0.8)
plt.axhline(y=best_test_acc, color='r', linestyle='--', alpha=0.7, label=f'Best: {best_test_acc:.4f}')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training Progress - Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('cnn_qnn_45k_results.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\nğŸ† Final Summary:")
print(f"Best accuracy: {best_test_acc:.4f}")
print(f"Total epochs: {len(train_accs)}")
print(f"Parameter efficiency: {best_test_acc/total_params*1000000:.2f} acc/1M params")

# ----------------------------------
# í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡  ë° CSV ì €ì¥
# ----------------------------------
from datetime import datetime

print(f"\nğŸ” Starting inference on test data...")

# ìµœê³  ëª¨ë¸ ë¡œë“œ
try:
    model.load_state_dict(torch.load('best_cnn_qnn_45k.pth'))
    print("âœ… Best model loaded")
except:
    print("âš ï¸ Using current model")

# í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” (ë°°ì¹˜ í¬ê¸° 1ë¡œ ê°œë³„ ì¶”ë¡ )
test_inference_loader = DataLoader(TensorDataset(test_x, test_y), 
                                  batch_size=1, shuffle=False)

model.eval()
all_preds, all_targets = [], []

with torch.no_grad():
    for data, target in tqdm(test_inference_loader, desc="Inference", 
                           total=len(test_inference_loader), leave=False):
        logits = model(data)
        # BCEWithLogitsLossë¥¼ ì‚¬ìš©í–ˆìœ¼ë¯€ë¡œ sigmoid ì ìš© í›„ 0.5 ê¸°ì¤€ìœ¼ë¡œ ì˜ˆì¸¡
        pred = (torch.sigmoid(logits) > 0.5).long().view(1)
        all_preds.append(pred.cpu())
        all_targets.append(target.view(-1).cpu())

y_pred = torch.cat(all_preds).numpy().astype(int)
y_true = torch.cat(all_targets).numpy().astype(int)

# 0Â·6 ë¼ë²¨ë§Œ í‰ê°€ (ìš°ë¦¬ ëª¨ë¸ì€ ì´ë¯¸ 0/6ë§Œ ì‚¬ìš©)
test_mask = (y_true == 0) | (y_true == 1)  # ìš°ë¦¬ ëª¨ë¸ì—ì„œëŠ” 0=T-shirt, 1=Shirt
print("Total samples:", len(y_true))
print("Target samples:", test_mask.sum())

# ëª¨ë¸ ê²°ê³¼ê°€ 1ì¸ ê²ƒì„ 6ìœ¼ë¡œ ë³€ê²½ (ì›ë³¸ Fashion-MNIST ë¼ë²¨ë¡œ ë³µì›)
y_pred_mapped = np.where(y_pred == 1, 6, y_pred)
y_true_mapped = np.where(y_true == 1, 6, y_true)

acc = (y_pred_mapped[test_mask] == y_true_mapped[test_mask]).mean()
print(f"Accuracy (labels 0/6 only): {acc:.4f}")

# í˜„ì¬ ì‹œê°ì„ "YYYYMMDD_HHMMSS" í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
now = datetime.now().strftime("%Y%m%d_%H%M%S")

# ì›ë³¸ íŒŒì¼ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒˆ íŒŒì¼ëª… ìƒì„±
y_pred_filename = f"qcnn_y_pred_{now}.csv"
np.savetxt(y_pred_filename, y_pred_mapped, fmt="%d")

print(f"âœ… Predictions saved to: {y_pred_filename}")
print(f"ğŸ“Š Prediction distribution: 0={np.sum(y_pred_mapped==0)}, 6={np.sum(y_pred_mapped==6)}")