import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torchvision import datasets, transforms

import pennylane as qml
from pennylane import numpy as np
import matplotlib.pyplot as plt
from tqdm.auto import tqdm
import math
import time
import json
import os
from typing import Dict, List, Optional

# -----------------------
# ì‹¤ì œ í…ŒìŠ¤íŠ¸ìš© Production ë²„ì „
# -----------------------

class ProductionPaperQNN(nn.Module):
    """
    ì‹¤ì œ í…ŒìŠ¤íŠ¸ìš© Paper QNN
    - ì„±ëŠ¥ ìµœì í™”
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
    - í™•ì¥ì„±
    - ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
    """
    
    def __init__(self, 
                 ensemble_type: str = "general",
                 n_qubits: int = 8,
                 frqi_iterations: int = 50,  # ì‹¤ì œ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì¶•ì†Œ
                 cache_limit: int = 1000):   # ë©”ëª¨ë¦¬ ê´€ë¦¬
        super().__init__()
        
        self.ensemble_type = ensemble_type
        self.n_qubits = n_qubits
        self.frqi_iterations = frqi_iterations
        self.cache_limit = cache_limit
        self.dev = qml.device("default.qubit", wires=self.n_qubits)
        
        # FRQI ìºì‹œ (ë©”ëª¨ë¦¬ ì œí•œ)
        self.frqi_cache = {}
        self.cache_access_count = {}
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.timing_stats = {
            'frqi_encoding': [],
            'quantum_forward': [],
            'total_forward': []
        }
        
        # ì–‘ì í…ì„œ íŠ¸ë ˆì¸ ë¶„ë¥˜ê¸° ë§¤ê°œë³€ìˆ˜ (ì¦ê°€)
        if ensemble_type == "general":
            params_per_gate = 14  # ì‹¤ì œ ì‚¬ìš©í•˜ëŠ” ë§¤ê°œë³€ìˆ˜ ìˆ˜
        else:
            params_per_gate = 6
        
        n_classifier_params = 3 * (self.n_qubits - 1) * params_per_gate + 20
        
        self.classifier_params = nn.Parameter(
            torch.randn(n_classifier_params, dtype=torch.float64) * 0.1
        )
        
        # ê°•í™”ëœ ê³ ì „ í›„ì²˜ë¦¬ ë„¤íŠ¸ì›Œí¬
        self.classical_head = nn.Sequential(
            nn.Linear(1, 32),
            nn.ReLU(),
            nn.BatchNorm1d(32),
            nn.Dropout(0.2),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(16, 1)
        ).double()
        
        print(f"ğŸš€ Production QNN initialized:")
        print(f"  - Ensemble: {ensemble_type}")
        print(f"  - Qubits: {self.n_qubits}")
        print(f"  - Classifier params: {n_classifier_params}")
        print(f"  - FRQI iterations: {frqi_iterations}")
        print(f"  - Cache limit: {cache_limit}")
    
    def _manage_cache(self):
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìºì‹œ ê´€ë¦¬"""
        if len(self.frqi_cache) > self.cache_limit:
            # LRU ë°©ì‹ìœ¼ë¡œ ìºì‹œ ì •ë¦¬
            sorted_items = sorted(
                self.cache_access_count.items(), 
                key=lambda x: x[1]
            )
            
            # ê°€ì¥ ì ê²Œ ì‚¬ìš©ëœ í•­ëª©ë“¤ ì œê±°
            items_to_remove = len(self.frqi_cache) - self.cache_limit + 100
            for key, _ in sorted_items[:items_to_remove]:
                if key in self.frqi_cache:
                    del self.frqi_cache[key]
                    del self.cache_access_count[key]
    
    def preprocess_image_to_frqi(self, image: torch.Tensor) -> torch.Tensor:
        """ìµœì í™”ëœ FRQI ì „ì²˜ë¦¬"""
        # 28x28ì„ 32x32ë¡œ íŒ¨ë”© (ë…¼ë¬¸ê³¼ ë™ì¼)
        if image.shape[-1] == 28:
            pad = (2, 2, 2, 2)
            image = torch.nn.functional.pad(image, pad, mode='constant', value=0)
        
        # íš¨ìœ¨ì ì¸ ì •ê·œí™”
        flat_image = image.flatten()
        img_min, img_max = flat_image.min(), flat_image.max()
        normalized = (flat_image - img_min) / (img_max - img_min + 1e-8)
        
        # FRQI ê°ë„ ë³€í™˜
        angles = normalized * math.pi / 2
        return angles[:self.n_qubits]
    
    def optimize_frqi_encoding_fast(self, image: torch.Tensor, image_id: str) -> torch.Tensor:
        """
        ê°œì„ ëœ FRQI ì¸ì½”ë”© (ë” ë‚˜ì€ ì´ë¯¸ì§€ í‘œí˜„)
        """
        start_time = time.time()
        
        # ìºì‹œ í™•ì¸
        if image_id in self.frqi_cache:
            self.cache_access_count[image_id] = self.cache_access_count.get(image_id, 0) + 1
            return self.frqi_cache[image_id]
        
        # ë” ë‚˜ì€ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ
        target_angles = self.preprocess_image_to_frqi(image)
        
        # ì´ë¯¸ì§€ì˜ í†µê³„ì  íŠ¹ì§• í™œìš©
        img_mean = torch.mean(target_angles)
        img_std = torch.std(target_angles)
        img_max = torch.max(target_angles)
        img_min = torch.min(target_angles)
        
        # ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ ì¸ì½”ë”© (í•™ìŠµ ê°€ëŠ¥í•˜ë„ë¡)
        n_encoding_params = self.n_qubits
        
        # ì´ë¯¸ì§€ì˜ ì£¼ìš” íŠ¹ì§•ì„ ì§ì ‘ ë§¤í•‘
        encoding_params = torch.zeros(n_encoding_params, dtype=torch.float64)
        
        # ì´ë¯¸ì§€ë¥¼ 8ê°œ ì˜ì—­ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê° ì˜ì—­ì˜ í‰ê· ê°’ ì‚¬ìš©
        img_2d = image.squeeze().reshape(28, 28) if image.dim() > 2 else image.reshape(28, 28)
        
        # 8ê°œ ì˜ì—­ì˜ í‰ê· ê°’ì„ ê°ë„ë¡œ ë³€í™˜
        regions = [
            img_2d[:14, :14].mean(),    # ì¢Œìƒ
            img_2d[:14, 14:].mean(),    # ìš°ìƒ  
            img_2d[14:, :14].mean(),    # ì¢Œí•˜
            img_2d[14:, 14:].mean(),    # ìš°í•˜
            img_2d[:, :14].mean(),      # ì¢Œì¸¡
            img_2d[:, 14:].mean(),      # ìš°ì¸¡
            img_2d[:14, :].mean(),      # ìƒë‹¨
            img_2d[14:, :].mean()       # í•˜ë‹¨
        ]
        
        for i in range(n_encoding_params):
            # ì •ê·œí™”ëœ ì˜ì—­ í‰ê· ì„ ê°ë„ë¡œ ë³€í™˜
            region_val = regions[i] if i < len(regions) else img_mean
            encoding_params[i] = (region_val + 1) * math.pi / 4  # [-1,1] -> [0,Ï€/2]
        
        # ìœ íš¨í•œ ê°ë„ ë²”ìœ„ë¡œ í´ë¨í•‘
        encoding_params = torch.clamp(encoding_params, 0, math.pi/2)
        
        # ìºì‹œ ì €ì¥ ë° ê´€ë¦¬
        self.frqi_cache[image_id] = encoding_params
        self.cache_access_count[image_id] = 1
        self._manage_cache()
        
        # ì„±ëŠ¥ ê¸°ë¡
        encoding_time = time.time() - start_time
        self.timing_stats['frqi_encoding'].append(encoding_time)
        
        return self.frqi_cache[image_id]
    
    def quantum_tensor_train_classifier_optimized(self, frqi_params, classifier_params):
        """ìµœì í™”ëœ ì–‘ì í…ì„œ íŠ¸ë ˆì¸ ë¶„ë¥˜ê¸°"""
        
        @qml.qnode(self.dev, interface="torch")
        def circuit():
            # 1. FRQI ì¸ì½”ë”©ëœ ìƒíƒœ ì¤€ë¹„
            for i in range(min(self.n_qubits, len(frqi_params))):
                qml.RY(frqi_params[i], wires=i)
            
            # 2. ê°•í™”ëœ ì–‘ì í…ì„œ íŠ¸ë ˆì¸ ë ˆì´ì–´ë“¤
            param_idx = 0
            n_layers = 2  # ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´ ì¶•ì†Œ
            
            for layer in range(n_layers):
                for i in range(self.n_qubits - 1):
                    if self.ensemble_type == "general":
                        if param_idx + 15 <= len(classifier_params):
                            gate_params = classifier_params[param_idx:param_idx + 15]
                            
                            # íš¨ìœ¨ì ì¸ SU(4) êµ¬í˜„
                            qml.RY(gate_params[0], wires=i)
                            qml.RZ(gate_params[1], wires=i)
                            qml.RY(gate_params[2], wires=i+1)
                            qml.RZ(gate_params[3], wires=i+1)
                            
                            qml.CNOT(wires=[i, i+1])
                            
                            qml.RY(gate_params[4], wires=i)
                            qml.RZ(gate_params[5], wires=i)
                            qml.RY(gate_params[6], wires=i+1)
                            qml.RZ(gate_params[7], wires=i+1)
                            
                            qml.CNOT(wires=[i+1, i])
                            
                            qml.RY(gate_params[8], wires=i)
                            qml.RZ(gate_params[9], wires=i)
                            qml.RY(gate_params[10], wires=i+1)
                            qml.RZ(gate_params[11], wires=i+1)
                            
                            # ì¶”ê°€ ì–½í˜
                            qml.CNOT(wires=[i, i+1])
                            
                            qml.RY(gate_params[12], wires=i)
                            qml.RY(gate_params[13], wires=i+1)
                            
                            param_idx += 14  # ë” ë§ì€ ë§¤ê°œë³€ìˆ˜ ì‚¬ìš©
                    
                    else:  # sparse ensemble
                        if param_idx + 6 <= len(classifier_params):
                            gate_params = classifier_params[param_idx:param_idx + 6]
                            
                            qml.RY(gate_params[0], wires=i)
                            qml.RZ(gate_params[1], wires=i)
                            qml.RY(gate_params[2], wires=i+1)
                            qml.RZ(gate_params[3], wires=i+1)
                            
                            # ì¡°ê±´ë¶€ CNOT
                            if abs(gate_params[4].item()) > 0.1:
                                qml.CNOT(wires=[i, i+1])
                            
                            qml.RY(gate_params[5], wires=i)
                            param_idx += 6
            
            # 3. íŒë…ì¸µ
            if param_idx < len(classifier_params):
                qml.RY(classifier_params[param_idx], wires=0)
            
            return qml.expval(qml.PauliZ(0))
        
        return circuit()
    
    def forward(self, x: torch.Tensor, image_ids: Optional[List[str]] = None) -> torch.Tensor:
        """ìµœì í™”ëœ ìˆœì „íŒŒ"""
        start_time = time.time()
        batch_size = x.size(0)
        results = []
        
        for i in range(batch_size):
            image = x[i]
            image_id = image_ids[i] if image_ids is not None else f"batch_{i}_{time.time()}"
            
            # 1. ë¹ ë¥¸ FRQI ì¸ì½”ë”©
            frqi_start = time.time()
            frqi_params = self.optimize_frqi_encoding_fast(image, image_id)
            frqi_time = time.time() - frqi_start
            
            # 2. ì–‘ì ë¶„ë¥˜
            quantum_start = time.time()
            quantum_output = self.quantum_tensor_train_classifier_optimized(
                frqi_params, self.classifier_params
            )
            quantum_time = time.time() - quantum_start
            
            results.append(quantum_output)
            
            # ì„±ëŠ¥ ê¸°ë¡
            self.timing_stats['quantum_forward'].append(quantum_time)
        
        # ë°°ì¹˜ ê²°ê³¼ ê²°í•©
        quantum_batch = torch.stack(results).unsqueeze(1)
        
        # ê³ ì „ í›„ì²˜ë¦¬
        final_output = self.classical_head(quantum_batch)
        
        # ì „ì²´ ì‹œê°„ ê¸°ë¡
        total_time = time.time() - start_time
        self.timing_stats['total_forward'].append(total_time)
        
        return final_output
    
    def get_performance_stats(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        stats = {}
        for key, times in self.timing_stats.items():
            if times:
                stats[key] = {
                    'mean': sum(times) / len(times),
                    'min': min(times),
                    'max': max(times),
                    'count': len(times)
                }
        
        stats['cache_info'] = {
            'size': len(self.frqi_cache),
            'limit': self.cache_limit,
            'hit_rate': len([c for c in self.cache_access_count.values() if c > 1]) / max(1, len(self.cache_access_count))
        }
        
        return stats
    
    def save_model(self, path: str):
        """ëª¨ë¸ ì €ì¥ (ìºì‹œ í¬í•¨)"""
        save_dict = {
            'model_state_dict': self.state_dict(),
            'frqi_cache': self.frqi_cache,
            'cache_access_count': self.cache_access_count,
            'config': {
                'ensemble_type': self.ensemble_type,
                'n_qubits': self.n_qubits,
                'frqi_iterations': self.frqi_iterations,
                'cache_limit': self.cache_limit
            },
            'performance_stats': self.get_performance_stats()
        }
        torch.save(save_dict, path)
        print(f"Model saved to {path}")
    
    def load_model(self, path: str):
        """ëª¨ë¸ ë¡œë“œ (ìºì‹œ í¬í•¨)"""
        checkpoint = torch.load(path)
        self.load_state_dict(checkpoint['model_state_dict'])
        self.frqi_cache = checkpoint.get('frqi_cache', {})
        self.cache_access_count = checkpoint.get('cache_access_count', {})
        print(f"Model loaded from {path}")
        print(f"Cache entries: {len(self.frqi_cache)}")


# -----------------------
# ì‹¤ì œ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë”
# -----------------------
def load_full_fashion_mnist():
    """ì „ì²´ Fashion-MNIST ë°ì´í„° ë¡œë“œ"""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.2860,), (0.3530,)),
    ])
    
    train_set = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)
    test_set = datasets.FashionMNIST('./data', train=False, download=True, transform=transform)

    def filter_binary(dataset):
        indices = [i for i, (_, y) in enumerate(dataset) if y in [0, 6]]  # T-shirt vs Shirt
        data = torch.stack([dataset[i][0] for i in indices])
        labels = torch.tensor([int(dataset[i][1] == 6) for i in indices])
        return data, labels

    return filter_binary(train_set), filter_binary(test_set)


def create_production_dataloaders(train_size: int = 5000, test_size: int = 1000, batch_size: int = 16):
    """ì‹¤ì œ í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ë¡œë” ìƒì„±"""
    (train_x, train_y), (test_x, test_y) = load_full_fashion_mnist()
    
    # ë°ì´í„° íƒ€ì… ë³€í™˜
    train_x = train_x.double()[:train_size]
    test_x = test_x.double()[:test_size]
    train_y = train_y.double().unsqueeze(1)[:train_size]
    test_y = test_y.double().unsqueeze(1)[:test_size]
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    train_loader = DataLoader(
        TensorDataset(train_x, train_y), 
        batch_size=batch_size, 
        shuffle=True,
        num_workers=0  # ì–‘ì íšŒë¡œì™€ í˜¸í™˜ì„±ì„ ìœ„í•´
    )
    
    test_loader = DataLoader(
        TensorDataset(test_x, test_y), 
        batch_size=batch_size,
        num_workers=0
    )
    
    return train_loader, test_loader


# -----------------------
# ì‹¤ì œ í…ŒìŠ¤íŠ¸ìš© í›ˆë ¨ í•¨ìˆ˜
# -----------------------
def train_production_model(model, train_loader, test_loader, epochs: int = 50):
    """ì‹¤ì œ í…ŒìŠ¤íŠ¸ìš© í›ˆë ¨ í•¨ìˆ˜"""
    
    # ê°œì„ ëœ ìµœì í™” ì„¤ì • (ë” ë¹ ë¥¸ í•™ìŠµ)
    optimizer = optim.Adam([
        {'params': model.classifier_params, 'lr': 0.01, 'weight_decay': 1e-4},  # í•™ìŠµë¥  10ë°° ì¦ê°€
        {'params': model.classical_head.parameters(), 'lr': 0.005, 'weight_decay': 1e-3}  # í•™ìŠµë¥  5ë°° ì¦ê°€
    ])
    
    # í´ë˜ìŠ¤ ë¶ˆê· í˜• ê³ ë ¤í•œ ì†ì‹¤ í•¨ìˆ˜
    criterion = nn.BCEWithLogitsLoss()
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)  # ë” ì ê·¹ì ì¸ ìŠ¤ì¼€ì¤„ë§
    
    # í›ˆë ¨ ê¸°ë¡
    train_losses, train_accs = [], []
    test_losses, test_accs = [], []
    best_test_acc = 0
    patience_counter = 0
    patience = 35
    
    print(f"\nğŸš€ Starting Production Training:")
    print(f"  - Epochs: {epochs}")
    print(f"  - Train batches: {len(train_loader)}")
    print(f"  - Test batches: {len(test_loader)}")
    
    for epoch in tqdm(range(epochs), desc="Training"):
        # í›ˆë ¨
        model.train()
        total_loss, total_acc = 0, 0
        
        for batch_idx, (xb, yb) in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}", leave=False)):
            optimizer.zero_grad()
            
            # ì´ë¯¸ì§€ í•´ì‹œ ê¸°ë°˜ ID (ìºì‹œ íš¨ìœ¨ì„±)
            image_ids = []
            for i in range(xb.size(0)):
                img_hash = hash(xb[i].flatten().sum().item())
                image_ids.append(f"img_{img_hash}")
            
            preds = model(xb, image_ids)
            loss = criterion(preds, yb)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            acc = ((torch.sigmoid(preds) > 0.5).double() == yb).double().mean()
            total_loss += loss.item()
            total_acc += acc.item()
        
        # í‰ê°€
        model.eval()
        test_loss, test_acc = 0, 0
        with torch.no_grad():
            for batch_idx, (xb, yb) in enumerate(test_loader):
                image_ids = []
                for i in range(xb.size(0)):
                    img_hash = hash(xb[i].flatten().sum().item())
                    image_ids.append(f"img_{img_hash}")
                preds = model(xb, image_ids)
                loss = criterion(preds, yb)
                acc = ((torch.sigmoid(preds) > 0.5).double() == yb).double().mean()
                test_loss += loss.item()
                test_acc += acc.item()
        
        # í‰ê·  ê³„ì‚°
        avg_train_loss = total_loss / len(train_loader)
        avg_train_acc = total_acc / len(train_loader)
        avg_test_loss = test_loss / len(test_loader)
        avg_test_acc = test_acc / len(test_loader)
        
        # ê¸°ë¡
        train_losses.append(avg_train_loss)
        train_accs.append(avg_train_acc)
        test_losses.append(avg_test_loss)
        test_accs.append(avg_test_acc)
        
        scheduler.step()  # StepLRì€ ì¸ì ë¶ˆí•„ìš”
        
        # ìµœê³  ì„±ëŠ¥ ì²´í¬
        if avg_test_acc > best_test_acc:
            best_test_acc = avg_test_acc
            patience_counter = 0
            model.save_model('best_production_qnn.pth')
        else:
            patience_counter += 1
        
        # ë¡œê¹…
        if epoch % 5 == 0 or patience_counter == 0:
            print(f"[Epoch {epoch+1}] Train Acc: {avg_train_acc:.4f} | Test Acc: {avg_test_acc:.4f} | Best: {best_test_acc:.4f}")
            
            # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
            stats = model.get_performance_stats()
            if 'total_forward' in stats:
                print(f"  Avg forward time: {stats['total_forward']['mean']:.3f}s")
                print(f"  Cache hit rate: {stats['cache_info']['hit_rate']:.2f}")
        
        # ì¡°ê¸° ì¢…ë£Œ
        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break
    
    return {
        'train_losses': train_losses,
        'train_accs': train_accs,
        'test_losses': test_losses,
        'test_accs': test_accs,
        'best_test_acc': best_test_acc,
        'performance_stats': model.get_performance_stats()
    }


# -----------------------
# ë©”ì¸ ì‹¤í–‰
# -----------------------
if __name__ == "__main__":
    print("ğŸš€ Production Paper QNN - Real Test Implementation")
    print("=" * 70)
    
    # ì „ì²´ ë°ì´í„° ì„¤ì •
    TRAIN_SIZE = 12000  # ì „ì²´ í›ˆë ¨ ë°ì´í„° (í´ë˜ìŠ¤ 0: 6000 + í´ë˜ìŠ¤ 6: 6000)
    TEST_SIZE = 2000    # ì „ì²´ í…ŒìŠ¤íŠ¸ ë°ì´í„° (í´ë˜ìŠ¤ 0: 1000 + í´ë˜ìŠ¤ 6: 1000)
    BATCH_SIZE = 32     # ë” í° ë°°ì¹˜ í¬ê¸°
    EPOCHS = 300        # ì „ì²´ ë°ì´í„°ì´ë¯€ë¡œ ì—í¬í¬ ìˆ˜ ì¶•ì†Œ
    
    print(f"ğŸ“Š Test Configuration:")
    print(f"  - Train size: {TRAIN_SIZE}")
    print(f"  - Test size: {TEST_SIZE}")
    print(f"  - Batch size: {BATCH_SIZE}")
    print(f"  - Epochs: {EPOCHS}")
    
    # ë°ì´í„° ë¡œë“œ
    train_loader, test_loader = create_production_dataloaders(
        train_size=TRAIN_SIZE, 
        test_size=TEST_SIZE, 
        batch_size=BATCH_SIZE
    )
     
    print(f"\nâœ… Data loaded successfully")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = ProductionPaperQNN(
        ensemble_type="general",
        n_qubits=8,
        frqi_iterations=30,  # ì‹¤ì œ í…ŒìŠ¤íŠ¸ìš©
        cache_limit=12000  # ì „ì²´ ë°ì´í„°ì— ë§ê²Œ ì¦ê°€
    )
    
    # ë§¤ê°œë³€ìˆ˜ ìˆ˜ ê³„ì‚°
    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nğŸ“ˆ Model Info:")
    print(f"  - Total parameters: {total_params:,}")
    print(f"  - Parameter limit check: {total_params <= 50000} (â‰¤50K)")
    
    # ì‹¤ì œ í›ˆë ¨ ì‹¤í–‰
    print(f"\nğŸ‹ï¸ Starting Real Training...")
    results = train_production_model(model, train_loader, test_loader, epochs=EPOCHS)
    
    # ìµœì¢… ê²°ê³¼
    print(f"\nğŸ¯ Final Results:")
    print(f"  - Best Test Accuracy: {results['best_test_acc']:.4f}")
    print(f"  - Total epochs: {len(results['train_accs'])}")
    
    # ì„±ëŠ¥ í†µê³„
    perf_stats = results['performance_stats']
    if 'total_forward' in perf_stats:
        print(f"  - Avg forward time: {perf_stats['total_forward']['mean']:.3f}s")
        print(f"  - Cache hit rate: {perf_stats['cache_info']['hit_rate']:.2f}")
        print(f"  - Cache size: {perf_stats['cache_info']['size']}")
    
    # ê²°ê³¼ ì €ì¥
    with open('production_results.json', 'w') as f:
        json.dump({
            'best_accuracy': results['best_test_acc'],
            'final_epoch': len(results['train_accs']),
            'performance_stats': perf_stats,
            'config': {
                'train_size': TRAIN_SIZE,
                'test_size': TEST_SIZE,
                'batch_size': BATCH_SIZE,
                'epochs': EPOCHS
            }
        }, f, indent=2)
    
    print(f"\nâœ… Production test completed!")
    print(f"Results saved to: production_results.json")
    print(f"Best model saved to: best_production_qnn.pth")