import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torchvision import datasets, transforms

import pennylane as qml
from pennylane import numpy as np
from sklearn.preprocessing import normalize
from tqdm.auto import tqdm
import matplotlib.pyplot as plt

# -----------------------
# ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# -----------------------
def load_fashion_mnist_binary():
    # ê°œì„ ëœ ë°ì´í„° ì „ì²˜ë¦¬
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.2860,), (0.3530,)),  # Fashion-MNIST ì‹¤ì œ í†µê³„ê°’
        transforms.RandomRotation(5),  # ë°ì´í„° ì¦ê°•
        transforms.RandomHorizontalFlip(0.1),  # ì•½ê°„ì˜ flip (ì˜ë¥˜ íŠ¹ì„±ìƒ ì œí•œì )
    ])
    
    # í…ŒìŠ¤íŠ¸ìš© ë³€í™˜ (ì¦ê°• ì—†ìŒ)
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.2860,), (0.3530,))
    ])
    
    train_set = datasets.FashionMNIST('./data', train=True, download=True, transform=transform)
    test_set = datasets.FashionMNIST('./data', train=False, download=True, transform=test_transform)

    def filter(dataset):
        indices = [i for i, (_, y) in enumerate(dataset) if y in [0, 6]]
        data = torch.stack([dataset[i][0] for i in indices])
        labels = torch.tensor([int(dataset[i][1] == 6) for i in indices])  # 0 â†’ 0, 6 â†’ 1
        return data, labels

    return filter(train_set), filter(test_set)

(train_x, train_y), (test_x, test_y) = load_fashion_mnist_binary()

# í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
print(f"Train data - Class 0: {(train_y == 0).sum()}, Class 1: {(train_y == 1).sum()}")
print(f"Test data - Class 0: {(test_y == 0).sum()}, Class 1: {(test_y == 1).sum()}")

# í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
print(f"Train data - Class 0: {(train_y == 0).sum()}, Class 1: {(train_y == 1).sum()}")
print(f"Test data - Class 0: {(test_y == 0).sum()}, Class 1: {(test_y == 1).sum()}")

# -----------------------
# QNN ì •ì˜ (8-qubit í’ë¶€í•œ Quantum Circuit)
# -----------------------
n_qubits = 8  # 8-qubit ì‹œìŠ¤í…œ
n_qnn_params = 80  # 10ê°œ ë ˆì´ì–´ë¡œ ë§¤ìš° ê¹Šì€ circuit (ì–‘ì í‘œí˜„ë ¥ ê·¹ëŒ€í™”)
dev = qml.device("default.qubit", wires=n_qubits)

# ê·¹ë„ë¡œ ê¹Šì€ 8-qubit Quantum Circuit (80 íŒŒë¼ë¯¸í„°, 10 layers)
@qml.qnode(dev, interface="torch")
def quantum_circuit(inputs, qnn_params):
    # Data encoding - ë§¤ìš° ê°•í™”ëœ ì¸ì½”ë”©
    for i in range(n_qubits):
        qml.H(wires=i)
        qml.RZ(2.*inputs[i], wires=i)
        qml.RY(inputs[i], wires=i)
        qml.RX(0.5*inputs[i], wires=i)  # ì¶”ê°€ ì¸ì½”ë”©
    
    # Layer 1: RY rotations + full entanglement
    for i in range(n_qubits):
        qml.RY(2.*qnn_params[i], wires=i)
    for i in range(n_qubits - 1):
        qml.CNOT(wires=[i, i+1])
    qml.CNOT(wires=[7, 0])  # circular
    
    # Layer 2: RX rotations + alternating entanglement
    for i in range(n_qubits):
        qml.RX(2.*qnn_params[8 + i], wires=i)
    for i in range(0, n_qubits-1, 2):
        qml.CNOT(wires=[i, i+1])
    for i in range(1, n_qubits-2, 2):
        qml.CNOT(wires=[i, i+2])
    
    # Layer 3: RZ rotations + long-range entanglement
    for i in range(n_qubits):
        qml.RZ(2.*qnn_params[16 + i], wires=i)
    for i in range(n_qubits//2):
        qml.CNOT(wires=[i, i + n_qubits//2])
    
    # Layer 4: Second RY layer + reverse entanglement
    for i in range(n_qubits):
        qml.RY(2.*qnn_params[24 + i], wires=i)
    for i in range(n_qubits - 2, -1, -1):
        qml.CNOT(wires=[i+1, i])
    
    # Layer 5: Second RX layer + skip connections
    for i in range(n_qubits):
        qml.RX(2.*qnn_params[32 + i], wires=i)
    for i in range(0, n_qubits-2, 2):
        qml.CNOT(wires=[i, i+2])
    
    # Layer 6: Second RZ layer + diagonal entanglement
    for i in range(n_qubits):
        qml.RZ(2.*qnn_params[40 + i], wires=i)
    for i in range(n_qubits//2):
        qml.CNOT(wires=[i, (i + n_qubits//2) % n_qubits])
    
    # Layer 7: Third RY layer + complex pattern
    for i in range(n_qubits):
        qml.RY(2.*qnn_params[48 + i], wires=i)
    for i in range(n_qubits):
        qml.CNOT(wires=[i, (i + 3) % n_qubits])
    
    # Layer 8: Third RX layer + star pattern
    for i in range(n_qubits):
        qml.RX(2.*qnn_params[56 + i], wires=i)
    for i in range(1, n_qubits):
        qml.CNOT(wires=[0, i])  # star pattern from qubit 0
    
    # Layer 9: Third RZ layer + ring pattern
    for i in range(n_qubits):
        qml.RZ(2.*qnn_params[64 + i], wires=i)
    for i in range(n_qubits):
        qml.CNOT(wires=[i, (i + 1) % n_qubits])
    
    # Layer 10: Final variational layer
    for i in range(n_qubits):
        qml.RY(2.*qnn_params[72 + i], wires=i)
    
    # Multi-qubit measurement for maximum information extraction
    return qml.expval(qml.PauliZ(0) @ qml.PauliZ(1) @ qml.PauliZ(2) @ qml.PauliZ(3))

# QNN íŒŒë¼ë¯¸í„°ë¥¼ ì§ì ‘ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤
class QuantumLayer(nn.Module):
    def __init__(self, n_params):
        super().__init__()
        self.qnn_params = nn.Parameter(torch.randn(n_params, dtype=torch.float64) * 0.1)
        
    def forward(self, x):
        # ë°°ì¹˜ ì²˜ë¦¬
        results = []
        for i in range(x.size(0)):
            # 8ì°¨ì› ì…ë ¥ ì‚¬ìš©
            input_data = x[i]  # ì´ë¯¸ 8ì°¨ì›ì´ì–´ì•¼ í•¨
            result = quantum_circuit(input_data, self.qnn_params)
            results.append(result)
        return torch.stack(results)

qlayer = QuantumLayer(n_qnn_params)

# -----------------------
# ì •í™•íˆ 45K íŒŒë¼ë¯¸í„° ëª¨ë¸ ì„¤ê³„
# -----------------------
# ìµœì¢… ì •í™•í•œ 45K ëª¨ë¸ (ë¯¸ì„¸ ì¡°ì •)
class QCNN_Model(nn.Module):
    def __init__(self):
        super().__init__()
        # ëª©í‘œ: 50,000 íŒŒë¼ë¯¸í„° ì´í•˜
        # QNN: 80 íŒŒë¼ë¯¸í„° (8-qubit 10-layer ê·¹ë„ë¡œ ê¹Šì€ circuit)
        # CNN: íš¨ìœ¨ì ì¸ íŠ¹ì§• ì¶”ì¶œ
        # Classifier: QNN ê°•í™”ë¥¼ ìœ„í•´ ì¶•ì†Œ
        
        # ìµœì í™”ëœ CNN (50K ì œí•œ ë‚´ì—ì„œ ê· í˜•ì¡íŒ ì„¤ê³„)
        self.cnn = nn.Sequential(
            # 28x28 â†’ 14x14
            nn.Conv2d(1, 8, 5, stride=2, padding=2),     # params: 1*8*25 + 8 = 208
            nn.BatchNorm2d(8),                           # params: 8*2 = 16
            nn.ReLU(),
            nn.Dropout2d(0.1),
            
            # 14x14 â†’ 7x7
            nn.Conv2d(8, 16, 3, stride=2, padding=1),    # params: 8*16*9 + 16 = 1,168
            nn.BatchNorm2d(16),                          # params: 16*2 = 32
            nn.ReLU(),
            nn.Dropout2d(0.1),
            
            # 7x7 â†’ 4x4
            nn.Conv2d(16, 12, 3, stride=1, padding=1),   # params: 16*12*9 + 12 = 1,740
            nn.BatchNorm2d(12),                          # params: 12*2 = 24
            nn.ReLU(),
            nn.Dropout2d(0.1),
            
            # 4x4 â†’ 2x2
            nn.Conv2d(12, 8, 3, stride=2, padding=1),    # params: 12*8*9 + 8 = 872
            nn.BatchNorm2d(8),                           # params: 8*2 = 16
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(2),                     # 2x2x8 = 32
            
            nn.Flatten(),
            nn.Linear(32, 16),                           # params: 32*16 + 16 = 528
            nn.BatchNorm1d(16),                          # params: 16*2 = 32
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(16, 8),                            # params: 16*8 + 8 = 136 (8-qubit ì…ë ¥)
            nn.BatchNorm1d(8),                           # params: 8*2 = 16
            nn.ReLU(),
            nn.Dropout(0.1)
        ).double()
        
        self.qnn = qlayer
        
        # ì¶•ì†Œëœ ë¶„ë¥˜ê¸° (QNN ê°•í™”ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° ì¬ë¶„ë°°)
        self.classifier = nn.Sequential(
            nn.Linear(1, 150),      # params: 1*150 + 150 = 300
            nn.BatchNorm1d(150),    # params: 150*2 = 300
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(150, 75),     # params: 150*75 + 75 = 11,325
            nn.BatchNorm1d(75),     # params: 75*2 = 150
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(75, 35),      # params: 75*35 + 35 = 2,660
            nn.BatchNorm1d(35),     # params: 35*2 = 70
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(35, 15),      # params: 35*15 + 15 = 540
            nn.BatchNorm1d(15),     # params: 15*2 = 30
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(15, 1),       # params: 15*1 + 1 = 16
        ).double()
        
        # ì´ ê³„ì‚°: CNN(ì•½ 4,788) + QNN(80) + Classifier(ì•½ 15,391) = ì•½ 20,259
        # QNN ê°•í™”ë¡œ ì–‘ì í‘œí˜„ë ¥ ê·¹ëŒ€í™”, 50K ì œí•œ ì¤€ìˆ˜
        
    def forward(self, x):
        features = self.cnn(x)  # ì´ì œ 8ì°¨ì› ì¶œë ¥ (8-qubit QNN ì…ë ¥ì— ë§ì¶¤)
        
        # QNN ì²˜ë¦¬
        q_out = self.qnn(features).unsqueeze(1)  # [batch_size, 1]
        
        return self.classifier(q_out)

# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° í•¨ìˆ˜
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

# -----------------------
# í•™ìŠµ ì„¤ì •
# -----------------------
# ë°ì´í„°ë¥¼ double precisionìœ¼ë¡œ ë³€í™˜
train_x = train_x.double()
test_x = test_x.double()
train_y = train_y.double().unsqueeze(1)
test_y = test_y.double().unsqueeze(1)

# ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸°
train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=64, shuffle=True)
test_loader = DataLoader(TensorDataset(test_x, test_y), batch_size=64)

epochs = 300

# ëª¨ë¸ ì„ íƒ ë° íŒŒë¼ë¯¸í„° í™•ì¸
model = QCNN_Model()
total_params = count_parameters(model)

print(f"Model parameters: {total_params:,}")
print(f"Train data shape: {train_x.shape}, Test data shape: {test_x.shape}")

# 50K ì œí•œ í™•ì¸
if total_params <= 50000:
    print(f"âœ… Parameter limit satisfied: {total_params:,} â‰¤ 50,000")
else:
    print(f"âŒ Parameter limit exceeded: {total_params:,} > 50,000")

# QNN ê°•í™” ìµœì í™”ëœ í•™ìŠµë¥  ì„¤ì •
optimizer = optim.AdamW([
    {'params': model.cnn.parameters(), 'lr': 0.002, 'weight_decay': 1e-4},
    {'params': model.qnn.parameters(), 'lr': 0.008, 'weight_decay': 1e-7},  # QNN ë§¤ìš° ì ê·¹ì  í•™ìŠµ
    {'params': model.classifier.parameters(), 'lr': 0.003, 'weight_decay': 1e-4}
], betas=(0.9, 0.999), eps=1e-8)

# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° (ë¶ˆê· í˜• í•´ê²°)
class_counts = torch.bincount(train_y.long().flatten())
class_weights = 1.0 / class_counts.float()
class_weights = class_weights / class_weights.sum() * 2  # ì •ê·œí™”

print(f"Class weights: {class_weights}")

# ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì†ì‹¤ í•¨ìˆ˜
criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1]/class_weights[0])
# ë” ì ê·¹ì ì¸ ìŠ¤ì¼€ì¤„ë§
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=20, min_lr=1e-6)

# -----------------------
# í›ˆë ¨ ë£¨í”„
# -----------------------
best_test_acc = 0
patience_counter = 0
patience = 40  # ë” ë§ì€ ê¸°íšŒ ì œê³µ

train_losses, train_accs = [], []
test_losses, test_accs = [], []

print("Starting CNN-QNN training...")

for epoch in tqdm(range(epochs)):
    model.train()
    total_loss, total_acc = 0, 0
    
    for xb, yb in train_loader:
        optimizer.zero_grad()
        preds = model(xb)
        loss = criterion(preds, yb)
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        loss.backward()
        optimizer.step()
        
        acc = ((torch.sigmoid(preds) > 0.5).double() == yb).double().mean()
        total_loss += loss.item()
        total_acc += acc.item()

    # í‰ê°€
    model.eval()
    test_loss, test_acc = 0, 0
    with torch.no_grad():
        for xb, yb in test_loader:
            preds = model(xb)
            loss = criterion(preds, yb)
            acc = ((torch.sigmoid(preds) > 0.5).double() == yb).double().mean()
            test_loss += loss.item()
            test_acc += acc.item()
    
    avg_train_loss = total_loss / len(train_loader)
    avg_train_acc = total_acc / len(train_loader)
    avg_test_loss = test_loss / len(test_loader)
    avg_test_acc = test_acc / len(test_loader)
    
    # ê¸°ë¡
    train_losses.append(avg_train_loss)
    train_accs.append(avg_train_acc)
    test_losses.append(avg_test_loss)
    test_accs.append(avg_test_acc)
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
    scheduler.step(avg_test_acc)
    
    # ìµœê³  ì„±ëŠ¥ ì¶”ì 
    if avg_test_acc > best_test_acc:
        best_test_acc = avg_test_acc
        patience_counter = 0
        torch.save(model.state_dict(), 'best_cnn_qnn_45k.pth')
    else:
        patience_counter += 1
    
    if epoch % 15 == 0 or patience_counter == 0:
        print(f"[Epoch {epoch+1}] Train Acc: {avg_train_acc:.4f} | Test Acc: {avg_test_acc:.4f} | Best: {best_test_acc:.4f}")
    
    # ì¡°ê¸° ì¢…ë£Œ
    if patience_counter >= patience:
        print(f"Early stopping at epoch {epoch+1}")
        break

print(f"\nğŸ¯ Training complete! Best Test Accuracy: {best_test_acc:.4f}")

# ìƒì„¸í•œ ëª¨ë¸ ë¶„ì„
cnn_params = sum(p.numel() for p in model.cnn.parameters())
qnn_params = sum(p.numel() for p in model.qnn.parameters())
classifier_params = sum(p.numel() for p in model.classifier.parameters())

print(f"\nğŸ“Š Model Analysis:")
print(f"Total parameters: {total_params:,} / 50,000 ({total_params/50000*100:.1f}%)")
print(f"CNN parameters: {cnn_params:,} ({cnn_params/total_params*100:.1f}%)")
print(f"QNN parameters: {qnn_params:,} ({qnn_params/total_params*100:.1f}%)")
print(f"Classifier parameters: {classifier_params:,} ({classifier_params/total_params*100:.1f}%)")

print(f"\nâœ… Constraint Verification:")
print(f"Parameter limit (â‰¤50K): {total_params <= 50000} ({total_params:,})")
print(f"QNN parameters (8-60): {8 <= qnn_params <= 60} ({qnn_params})")
print(f"Hybrid model: âœ… (CNN + QNN + Classifier)")

# # ê²°ê³¼ ì‹œê°í™”
# plt.figure(figsize=(12, 4))

# plt.subplot(1, 2, 1)
# plt.plot(train_losses, label='Train Loss', alpha=0.8)
# plt.plot(test_losses, label='Test Loss', alpha=0.8)
# plt.xlabel('Epoch')
# plt.ylabel('Loss')
# plt.title('Training Progress - Loss')
# plt.legend()
# plt.grid(True, alpha=0.3)

# plt.subplot(1, 2, 2)
# plt.plot(train_accs, label='Train Accuracy', alpha=0.8)
# plt.plot(test_accs, label='Test Accuracy', alpha=0.8)
# plt.axhline(y=best_test_acc, color='r', linestyle='--', alpha=0.7, label=f'Best: {best_test_acc:.4f}')
# plt.xlabel('Epoch')
# plt.ylabel('Accuracy')
# plt.title('Training Progress - Accuracy')
# plt.legend()
# plt.grid(True, alpha=0.3)

# plt.tight_layout()
# plt.savefig('cnn_qnn_45k_results.png', dpi=300, bbox_inches='tight')
# plt.show()

print(f"\nğŸ† Final Summary:")
print(f"Best accuracy: {best_test_acc:.4f}")
print(f"Total epochs: {len(train_accs)}")
print(f"Parameter efficiency: {best_test_acc/total_params*1000000:.2f} acc/1M params")

# ----------------------------------
# ê¸°ì¡´ ë°©ì‹: 0,6 í´ë˜ìŠ¤ë§Œìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ í‰ê°€
# ----------------------------------
from datetime import datetime

print(f"\nğŸ” Starting inference on filtered test data (0,6 classes only)...")

# ìµœê³  ëª¨ë¸ ë¡œë“œ
try:
    model.load_state_dict(torch.load('best_cnn_qnn_45k.pth'))
    print("âœ… Best model loaded")
except:
    print("âš ï¸ Using current model")

# ê¸°ì¡´ í•„í„°ë§ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì„±ëŠ¥ í‰ê°€
test_inference_loader = DataLoader(TensorDataset(test_x, test_y), 
                                  batch_size=1, shuffle=False)

model.eval()
all_preds, all_targets = [], []

with torch.no_grad():
    for data, target in tqdm(test_inference_loader, desc="Filtered Test Inference", 
                           total=len(test_inference_loader), leave=False):
        logits = model(data)
        pred = (torch.sigmoid(logits) > 0.5).long().view(1)
        all_preds.append(pred.cpu())
        all_targets.append(target.view(-1).cpu())

y_pred_filtered = torch.cat(all_preds).numpy().astype(int)
y_true_filtered = torch.cat(all_targets).numpy().astype(int)

# í•„í„°ë§ëœ ë°ì´í„° ì„±ëŠ¥ í‰ê°€
y_pred_mapped_filtered = np.where(y_pred_filtered == 1, 6, y_pred_filtered)
y_true_mapped_filtered = np.where(y_true_filtered == 1, 6, y_true_filtered)

filtered_acc = (y_pred_mapped_filtered == y_true_mapped_filtered).mean()
print(f"ğŸ“Š Filtered Test Results (0,6 classes only):")
print(f"Total samples: {len(y_pred_filtered)}")
print(f"Accuracy: {filtered_acc:.4f}")
print(f"Prediction distribution: 0={np.sum(y_pred_mapped_filtered==0)}, 6={np.sum(y_pred_mapped_filtered==6)}")

# ----------------------------------
# ìµœì¢… CSV ìƒì„±: ì „ì²´ 10,000ê°œ í…ŒìŠ¤íŠ¸ì…‹ ì²˜ë¦¬
# ----------------------------------
print(f"\nğŸ” Generating final CSV with full 10,000 test samples...")

# ì „ì²´ Fashion-MNIST í…ŒìŠ¤íŠ¸ì…‹ ë¡œë“œ (í•„í„°ë§ ì—†ìŒ)
full_test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.2860,), (0.3530,))
])
full_test_set = datasets.FashionMNIST('./data', train=False, download=False, transform=full_test_transform)

# ì „ì²´ 10,000ê°œ ì˜ˆì¸¡ ë°°ì—´ ì´ˆê¸°í™”
final_predictions = np.zeros(len(full_test_set), dtype=int)

print(f"Processing {len(full_test_set)} samples for final CSV...")

# ê° ìƒ˜í”Œì„ ê°œë³„ì ìœ¼ë¡œ ì²˜ë¦¬
for idx in tqdm(range(len(full_test_set)), desc="Final CSV Generation"):
    data, true_label = full_test_set[idx]
    
    if true_label in [0, 6]:  # T-shirt(0) ë˜ëŠ” Shirt(6)ë§Œ ëª¨ë¸ë¡œ ì¶”ë¡ 
        # ëª¨ë¸ ì¶”ë¡ 
        data_batch = data.unsqueeze(0).double()  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
        with torch.no_grad():
            logits = model(data_batch)
            pred = (torch.sigmoid(logits) > 0.5).long().item()
        
        # ëª¨ë¸ ì¶œë ¥ ë§¤í•‘: 0â†’0, 1â†’6
        final_predictions[idx] = 6 if pred == 1 else 0
    else:
        # 0,6ì´ ì•„ë‹Œ í´ë˜ìŠ¤ëŠ” ì›ë˜ ë¼ë²¨ ê·¸ëŒ€ë¡œ ìœ ì§€
        final_predictions[idx] = true_label

# ìµœì¢… ê²€ì¦: 0,6 í´ë˜ìŠ¤ì— ëŒ€í•œ ì •í™•ë„ í™•ì¸
full_true_labels = np.array([full_test_set[i][1] for i in range(len(full_test_set))])
eval_mask = (full_true_labels == 0) | (full_true_labels == 6)

final_acc = (final_predictions[eval_mask] == full_true_labels[eval_mask]).mean()
print(f"\nğŸ“Š Final CSV Validation:")
print(f"Total 0/6 samples in full dataset: {eval_mask.sum()}")
print(f"Accuracy on 0/6 classes: {final_acc:.4f}")

# CSV íŒŒì¼ ì €ì¥
now = datetime.now().strftime("%Y%m%d_%H%M%S")
csv_filename = f"qcnn_y_pred_full_{now}.csv"
np.savetxt(csv_filename, final_predictions, fmt="%d")

print(f"\nâœ… Final CSV saved: {csv_filename}")
print(f"ğŸ“Š Final prediction distribution:")
for class_idx in range(10):
    count = np.sum(final_predictions == class_idx)
    print(f"  Class {class_idx}: {count} samples")

print(f"\nğŸ¯ Summary:")
print(f"- Filtered test accuracy (training validation): {filtered_acc:.4f}")
print(f"- Full dataset accuracy (0/6 classes only): {final_acc:.4f}")
print(f"- CSV contains {len(final_predictions)} predictions")