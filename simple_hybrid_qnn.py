import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torchvision import datasets, transforms
from tqdm import tqdm
import matplotlib.pyplot as plt

import pennylane as qml

# ----------------------------------
# Step 1: Fashion-MNIST ë°ì´í„° ë¡œë“œ
# ----------------------------------
def load_fashion_mnist_binary():
    """T-shirt/top (0) vs Shirt (6) ì´ì§„ ë¶„ë¥˜ ë°ì´í„°"""
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    
    train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)
    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)
    
    def filter_classes(dataset, target_classes=[0, 6]):
        indices = [i for i, (_, label) in enumerate(dataset) if label in target_classes]
        data = torch.stack([dataset[i][0] for i in indices])
        labels = torch.tensor([dataset[i][1] for i in indices])
        labels = (labels == 6).long()  # T-shirt/top: 0, Shirt: 1
        return data, labels
    
    return filter_classes(train_dataset), filter_classes(test_dataset)

# ë°ì´í„° ë¡œë“œ
(train_data, train_labels), (test_data, test_labels) = load_fashion_mnist_binary()
print(f"Train shape: {train_data.shape}, Test shape: {test_data.shape}")

# ----------------------------------
# Step 2: ì–‘ì íšŒë¡œ ì •ì˜ (ê°„ë‹¨í•˜ê³  ì•ˆì „)
# ----------------------------------
n_qubits = 8
n_layers = 4
dev = qml.device("default.qubit", wires=n_qubits)

def quantum_layer(weights):
    """ê°„ë‹¨í•œ ì–‘ì ë ˆì´ì–´"""
    for i in range(n_qubits):
        qml.RY(weights[i], wires=i)
    for i in range(n_qubits - 1):
        qml.CNOT(wires=[i, i+1])

@qml.qnode(dev, interface="torch")
def quantum_circuit(inputs, weights):
    """ì–‘ì íšŒë¡œ - Amplitude Embedding + Variational Layers"""
    # Amplitude embedding (256D â†’ 8 qubits)
    qml.AmplitudeEmbedding(inputs, wires=range(n_qubits), normalize=True)
    
    # Variational layers
    for layer in range(n_layers):
        layer_weights = weights[layer * n_qubits:(layer + 1) * n_qubits]
        quantum_layer(layer_weights)
    
    # ë‹¤ì¤‘ ì¸¡ì • (ë” í’ë¶€í•œ ì •ë³´)
    return [qml.expval(qml.PauliZ(i)) for i in range(4)]

# ì–‘ì ë ˆì´ì–´ ìƒì„±
weight_shapes = {"weights": (n_layers * n_qubits,)}
qlayer = qml.qnn.TorchLayer(quantum_circuit, weight_shapes)

print(f"Quantum parameters: {n_layers * n_qubits}")

# ----------------------------------
# Step 3: ì§„ì •í•œ í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì •ì˜
# ----------------------------------
class SimpleHybridQNN(nn.Module):
    def __init__(self):
        super().__init__()
        
        # ê³ ì „ ê²½ë¡œ: CNN íŠ¹ì„± ì¶”ì¶œê¸°
        self.classical_path = nn.Sequential(
            # 28x28 â†’ 14x14
            nn.Conv2d(1, 16, kernel_size=5, stride=2, padding=2),
            nn.ReLU(),
            nn.BatchNorm2d(16),
            
            # 14x14 â†’ 7x7
            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.BatchNorm2d(32),
            
            # 7x7 â†’ 4x4
            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(2),  # 64x2x2 = 256
            
            nn.Flatten(),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 64),  # ê³ ì „ íŠ¹ì„± 64ì°¨ì›
            nn.ReLU()
        ).double()
        
        # ì–‘ì ê²½ë¡œ: QNN
        self.quantum_path = qlayer
        
        # íŠ¹ì„± ê²°í•© ë° ìµœì¢… ë¶„ë¥˜
        self.fusion_classifier = nn.Sequential(
            nn.Linear(64 + 4, 128),  # ê³ ì „(64) + ì–‘ì(4) = 68ì°¨ì›
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.4),
            
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Dropout(0.3),
            
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(32, 1),
            nn.Sigmoid()
        ).double()
        
        # ì–‘ì ì…ë ¥ì„ ìœ„í•œ ì°¨ì› ì¶•ì†Œ
        self.quantum_preprocessor = nn.Sequential(
            nn.Flatten(),  # 28*28 = 784
            nn.Linear(784, 512),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(512, 256),  # 256ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œ (2^8 = 256 â†’ 8 qubits)
            nn.Tanh()  # [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”
        ).double()

    def forward(self, x):
        batch_size = x.shape[0]
        
        # ê³ ì „ ê²½ë¡œ: CNNìœ¼ë¡œ ê³ ì°¨ì› íŠ¹ì„± ì¶”ì¶œ
        classical_features = self.classical_path(x)  # [B, 64]
        
        # ì–‘ì ê²½ë¡œ: ì´ë¯¸ì§€ â†’ 256D â†’ QNN
        quantum_input = self.quantum_preprocessor(x)  # [B, 256]
        
        # ë°°ì¹˜ë³„ ì–‘ì ì²˜ë¦¬
        quantum_outputs = []
        for i in range(batch_size):
            # L2 ì •ê·œí™” (Amplitude Embedding ìš”êµ¬ì‚¬í•­)
            normalized_input = torch.nn.functional.normalize(quantum_input[i], p=2, dim=0)
            q_out = self.quantum_path(normalized_input)
            
            # ë¦¬ìŠ¤íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜
            if isinstance(q_out, (list, tuple)):
                q_tensor = torch.stack(q_out)
            else:
                q_tensor = q_out
            quantum_outputs.append(q_tensor)
        
        quantum_features = torch.stack(quantum_outputs)  # [B, 4]
        
        # íŠ¹ì„± ìœµí•©: ê³ ì „ + ì–‘ì
        fused_features = torch.cat([classical_features, quantum_features], dim=1)  # [B, 68]
        
        # ìµœì¢… ë¶„ë¥˜
        output = self.fusion_classifier(fused_features)
        
        return output

# ----------------------------------
# Step 4: ëª¨ë¸ ì´ˆê¸°í™” ë° í›ˆë ¨ ì„¤ì •
# ----------------------------------
model = SimpleHybridQNN()

# íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°
total_params = sum(p.numel() for p in model.parameters())
classical_params = sum(p.numel() for p in model.classical_path.parameters()) + \
                  sum(p.numel() for p in model.quantum_preprocessor.parameters())
quantum_params = sum(p.numel() for p in model.quantum_path.parameters())
fusion_params = sum(p.numel() for p in model.fusion_classifier.parameters())

print(f"\nğŸ“Š Model Analysis:")
print(f"Total parameters: {total_params:,}")
print(f"Classical parameters: {classical_params:,}")
print(f"Quantum parameters: {quantum_params:,}")
print(f"Fusion parameters: {fusion_params:,}")
print(f"Quantum/Total ratio: {quantum_params/total_params:.3f}")

# 50K ì œí•œ í™•ì¸
if total_params <= 50000:
    print(f"âœ… Parameter limit satisfied: {total_params:,} â‰¤ 50,000")
else:
    print(f"âŒ Parameter limit exceeded: {total_params:,} > 50,000")

# ----------------------------------
# Step 5: ë°ì´í„° ì¤€ë¹„ ë° í›ˆë ¨
# ----------------------------------
# ë°ì´í„°ë¥¼ double precisionìœ¼ë¡œ ë³€í™˜
train_data = train_data.double()
test_data = test_data.double()
train_labels = train_labels.double().unsqueeze(1)
test_labels = test_labels.double().unsqueeze(1)

# DataLoader
batch_size = 16  # ì–‘ì ì²˜ë¦¬ ë•Œë¬¸ì— ì‘ì€ ë°°ì¹˜ ì‚¬ìš©
train_loader = DataLoader(TensorDataset(train_data, train_labels), 
                         batch_size=batch_size, shuffle=True)
test_loader = DataLoader(TensorDataset(test_data, test_labels), 
                        batch_size=batch_size, shuffle=False)

# ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜
criterion = nn.BCELoss()
optimizer = optim.AdamW([
    {'params': model.classical_path.parameters(), 'lr': 0.001, 'weight_decay': 1e-4},
    {'params': model.quantum_path.parameters(), 'lr': 0.01, 'weight_decay': 1e-5},
    {'params': model.fusion_classifier.parameters(), 'lr': 0.005, 'weight_decay': 1e-4},
    {'params': model.quantum_preprocessor.parameters(), 'lr': 0.002, 'weight_decay': 1e-4}
])

scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.8, patience=10)

# ----------------------------------
# Step 6: í›ˆë ¨ ë£¨í”„
# ----------------------------------
epochs = 80
best_test_acc = 0
patience_counter = 0
patience = 15

train_losses, train_accs = [], []
test_losses, test_accs = [], []

print("\nğŸš€ Starting Hybrid QNN Training...")

for epoch in range(epochs):
    # í›ˆë ¨
    model.train()
    total_loss, total_acc = 0, 0
    
    for xb, yb in tqdm(train_loader, desc=f"Epoch {epoch+1}", leave=False):
        optimizer.zero_grad()
        pred = model(xb)
        loss = criterion(pred, yb)
        
        # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        loss.backward()
        optimizer.step()
        
        acc = (pred.round() == yb).double().mean()
        total_loss += loss.item()
        total_acc += acc.item()

    # í‰ê°€
    model.eval()
    test_loss, test_acc = 0, 0
    with torch.no_grad():
        for xb, yb in test_loader:
            pred = model(xb)
            loss = criterion(pred, yb)
            acc = (pred.round() == yb).double().mean()
            test_loss += loss.item()
            test_acc += acc.item()
    
    # í‰ê·  ê³„ì‚°
    avg_train_loss = total_loss / len(train_loader)
    avg_train_acc = total_acc / len(train_loader)
    avg_test_loss = test_loss / len(test_loader)
    avg_test_acc = test_acc / len(test_loader)
    
    # ê¸°ë¡
    train_losses.append(avg_train_loss)
    train_accs.append(avg_train_acc)
    test_losses.append(avg_test_loss)
    test_accs.append(avg_test_acc)
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
    scheduler.step(avg_test_acc)
    
    # ìµœê³  ì„±ëŠ¥ ì¶”ì 
    if avg_test_acc > best_test_acc:
        best_test_acc = avg_test_acc
        patience_counter = 0
        torch.save(model.state_dict(), 'best_simple_hybrid_qnn.pth')
    else:
        patience_counter += 1
    
    if epoch % 10 == 0 or patience_counter == 0:
        print(f"[Epoch {epoch+1}] Train Acc: {avg_train_acc:.4f} | Test Acc: {avg_test_acc:.4f} | Best: {best_test_acc:.4f}")
    
    # ì¡°ê¸° ì¢…ë£Œ
    if patience_counter >= patience:
        print(f"Early stopping at epoch {epoch+1}")
        break

print(f"\nğŸ¯ Training complete! Best Test Accuracy: {best_test_acc:.4f}")

# ----------------------------------
# Step 7: ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
# ----------------------------------
# ìµœê³  ëª¨ë¸ ë¡œë“œ
try:
    model.load_state_dict(torch.load('best_simple_hybrid_qnn.pth'))
    print("âœ… Best model loaded")
except:
    print("âš ï¸ Using current model")

# ìµœì¢… í‰ê°€
model.eval()
final_test_acc = 0
all_preds, all_labels = [], []

with torch.no_grad():
    for xb, yb in test_loader:
        pred = model(xb)
        acc = (pred.round() == yb).double().mean()
        final_test_acc += acc.item()
        
        all_preds.extend(pred.cpu().numpy())
        all_labels.extend(yb.cpu().numpy())

final_test_acc /= len(test_loader)

# ë¶„ë¥˜ ì„±ëŠ¥ ë¶„ì„
from sklearn.metrics import classification_report, confusion_matrix

all_preds = torch.tensor(all_preds)
all_labels = torch.tensor(all_labels)
pred_classes = (all_preds > 0.5).int()

print(f"\nğŸ¯ Final Test Accuracy: {final_test_acc:.4f}")
print(f"\nğŸ“Š Classification Report:")
print(classification_report(all_labels.numpy(), pred_classes.numpy(), 
                          target_names=['T-shirt/top', 'Shirt']))

print(f"\nğŸ” Confusion Matrix:")
cm = confusion_matrix(all_labels.numpy(), pred_classes.numpy())
print(cm)

# ì‹œê°í™”
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(train_losses, label='Train Loss', alpha=0.8)
plt.plot(test_losses, label='Test Loss', alpha=0.8)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Progress - Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(train_accs, label='Train Accuracy', alpha=0.8)
plt.plot(test_accs, label='Test Accuracy', alpha=0.8)
plt.axhline(y=best_test_acc, color='r', linestyle='--', alpha=0.7, 
           label=f'Best: {best_test_acc:.4f}')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.title('Training Progress - Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('simple_hybrid_qnn_results.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\nğŸ† Final Summary:")
print(f"Best accuracy: {best_test_acc:.4f}")
print(f"Final accuracy: {final_test_acc:.4f}")
print(f"Total epochs: {len(train_accs)}")
print(f"Model efficiency: {final_test_acc/total_params*1000000:.2f} acc/1M params")

print(f"\nâœ… Hybrid Architecture Verification:")
print(f"Classical pathway: âœ… (CNN feature extraction)")
print(f"Quantum pathway: âœ… (QNN processing)")
print(f"Feature fusion: âœ… (Classical + Quantum â†’ Final classifier)")
print(f"Parameter distribution: Classical({classical_params:,}) + Quantum({quantum_params:,}) + Fusion({fusion_params:,})")