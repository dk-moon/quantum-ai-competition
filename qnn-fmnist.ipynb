{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c379e6",
   "metadata": {},
   "source": [
    "## 문제 설명\n",
    "\n",
    "Fasion-MNIST 데이터셋은 10개의 의류 카테고리로 구성된 흑백 이미지 데이터셋입니다. 본 예선 문제에서는 아래 두 클래스를 구분하는 **이진 분류기**를 양자 알고리즘을 활용하여 설계하는 것이 목표입니다. 아래의 이미지에서 T-shirt/top과 Shirt 라벨에 대해서만 분류하는 것입니다. 만들어야 할 모델은 양자 회로(quantum circuit)를 기반으로 하며, 잡음 없는 양자 시뮬레이터에서 실행가능해야 합니다.\n",
    "- 클래스 0 : **T-shirt/top**\n",
    "- 클래스 6 : **Shirt**\n",
    "\n",
    "## 제약 조건\n",
    "본 예선에서는 양자 모델 설계에 대한 현실적인 자원 제약을 고려하며, 다음과 같은 기술적 제한을 준수하여야 합니다.\n",
    "- 양자 개발 프레임워크는 PennyLane을 사용합니다.\n",
    "- 참가자는 최대 8 큐빗까지 사용 가능합니다.\n",
    "- 사용되는 큐빗 수는 모델 전반에 걸쳐 유지되어야 하며, ancilla qubit 또는 mid-circuit measurement는 허용되지 않습니다.\n",
    "- 전체 회로의 깊이(depth)는 최대 30으로 제한합니다.\n",
    "- 모델 내 학습 가능한 퀀텀 레이어 파라미터(num_trainable_params)의 총 개수는 8개 이상 ~ 60개 이하로 제한합니다.\n",
    "- 입력 데이터를 양자 상태로 인코딩하는 과정은 Amplitude Encoding, Angle Encoding, IQP-style Embedding 등을 자유롭게 활용할 수 있습니다.\n",
    "- 참가자의 판단에 따라 데이터 차원 축소가 필요한 경우, 비지도 차원 축소 기법(PCA 등)의 사용을 허가합니다.\n",
    "- 고전 머신러닝·딥러닝 모델과의 하이브리드 구성을 허용하되, 양자·고전 파라미터를 모두 합한 총 학습 가능 파라미터 수는 50,000(50K)개 이하로 제한합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b88d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion-MNIST 데이터 로드 및 전처리\n",
    "def load_fashion_mnist_binary():\n",
    "    # Fashion-MNIST 데이터셋 로드\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    \n",
    "    # T-shirt/top (0)과 Shirt (6) 클래스만 필터링\n",
    "    def filter_classes(dataset, target_classes=[0, 6]):\n",
    "        indices = [i for i, (_, label) in enumerate(dataset) if label in target_classes]\n",
    "        data = torch.stack([dataset[i][0] for i in indices])\n",
    "        labels = torch.tensor([dataset[i][1] for i in indices])\n",
    "        # 라벨을 0, 1로 변경 (0: T-shirt/top, 1: Shirt)\n",
    "        labels = (labels == 6).long()\n",
    "        return data, labels\n",
    "    \n",
    "    train_data, train_labels = filter_classes(train_dataset)\n",
    "    test_data, test_labels = filter_classes(test_dataset)\n",
    "    \n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "# 데이터 로드\n",
    "train_data, train_labels, test_data, test_labels = load_fashion_mnist_binary()\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "print(f\"Train labels distribution: {torch.bincount(train_labels)}\")\n",
    "print(f\"Test labels distribution: {torch.bincount(test_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 차원 축소 (PCA 사용)\n",
    "def preprocess_data(train_data, test_data, n_components=8):\n",
    "    # 데이터를 평면화\n",
    "    train_flat = train_data.view(train_data.shape[0], -1).numpy()\n",
    "    test_flat = test_data.view(test_data.shape[0], -1).numpy()\n",
    "    \n",
    "    # 표준화\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train_flat)\n",
    "    test_scaled = scaler.transform(test_flat)\n",
    "    \n",
    "    # PCA로 차원 축소 (8차원으로 축소하여 8 큐빗에 맞춤)\n",
    "    pca = PCA(n_components=n_components)\n",
    "    train_pca = pca.fit_transform(train_scaled)\n",
    "    test_pca = pca.transform(test_scaled)\n",
    "    \n",
    "    # 정규화 [-π, π] 범위로\n",
    "    train_normalized = np.pi * (train_pca - train_pca.min()) / (train_pca.max() - train_pca.min()) - np.pi/2\n",
    "    test_normalized = np.pi * (test_pca - train_pca.min()) / (train_pca.max() - train_pca.min()) - np.pi/2\n",
    "    \n",
    "    print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"Total explained variance: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "    \n",
    "    return torch.tensor(train_normalized, dtype=torch.float64), torch.tensor(test_normalized, dtype=torch.float64)\n",
    "\n",
    "# 데이터 전처리\n",
    "train_processed, test_processed = preprocess_data(train_data, test_data)\n",
    "print(f\"Processed train data shape: {train_processed.shape}\")\n",
    "print(f\"Processed test data shape: {test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantum_circuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 양자 회로 정의\n",
    "n_qubits = 8\n",
    "n_layers = 3  # 회로 깊이 제한을 고려\n",
    "\n",
    "# 양자 디바이스 설정\n",
    "dev = qml.device('default.qubit', wires=n_qubits)\n",
    "\n",
    "def angle_encoding(inputs, wires):\n",
    "    \"\"\"Angle encoding for input data\"\"\"\n",
    "    for i, wire in enumerate(wires):\n",
    "        qml.RY(inputs[i], wires=wire)\n",
    "\n",
    "def variational_layer(params, wires):\n",
    "    \"\"\"Variational layer with parameterized gates\"\"\"\n",
    "    # Single qubit rotations\n",
    "    for i, wire in enumerate(wires):\n",
    "        qml.RY(params[i], wires=wire)\n",
    "    \n",
    "    # Entangling gates (circular connectivity)\n",
    "    for i in range(len(wires)):\n",
    "        qml.CNOT(wires=[wires[i], wires[(i + 1) % len(wires)]])\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def quantum_circuit(inputs, params):\n",
    "    \"\"\"Complete quantum circuit\"\"\"\n",
    "    wires = range(n_qubits)\n",
    "    \n",
    "    # Data encoding\n",
    "    angle_encoding(inputs, wires)\n",
    "    \n",
    "    # Variational layers\n",
    "    for layer in range(n_layers):\n",
    "        layer_params = params[layer * n_qubits:(layer + 1) * n_qubits]\n",
    "        variational_layer(layer_params, wires)\n",
    "    \n",
    "    # Measurement\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# 파라미터 개수 확인\n",
    "total_params = n_layers * n_qubits\n",
    "print(f\"Total trainable parameters: {total_params}\")\n",
    "print(f\"Circuit depth estimate: ~{n_layers * 2 + 1}\")\n",
    "\n",
    "# 회로 정보 출력\n",
    "dummy_input = torch.zeros(n_qubits)\n",
    "dummy_params = torch.zeros(total_params)\n",
    "print(\"\\nQuantum circuit created successfully!\")\n",
    "print(qml.draw(quantum_circuit)(dummy_input, dummy_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantum_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 양자 모델 래퍼\n",
    "class QuantumClassifier(nn.Module):\n",
    "    def __init__(self, n_qubits, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # 학습 가능한 파라미터\n",
    "        self.params = nn.Parameter(torch.randn(n_layers * n_qubits) * 0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            output = quantum_circuit(x[i], self.params)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        outputs = torch.stack(outputs)\n",
    "        # 시그모이드를 적용하여 확률로 변환\n",
    "        return torch.sigmoid(outputs)\n",
    "\n",
    "# 모델 초기화\n",
    "model = QuantumClassifier(n_qubits, n_layers)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 설정\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_dataset = TensorDataset(train_processed, train_labels.float())\n",
    "test_dataset = TensorDataset(test_processed, test_labels.float())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 손실 함수와 옵티마이저\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 훈련 함수\n",
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        predicted = (output > 0.5).double()\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "    \n",
    "    return total_loss / len(train_loader), correct / total\n",
    "\n",
    "# 평가 함수\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predicted = (output > 0.5).double()\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    return total_loss / len(test_loader), correct / total\n",
    "\n",
    "print(\"Training setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타입 호환성 문제 해결\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# 데이터 로더 재생성 (double precision)\n",
    "train_dataset = TensorDataset(train_processed, train_labels.double())\n",
    "test_dataset = TensorDataset(test_processed, test_labels.double())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# QuantumClassifier 클래스 정의\n",
    "class QuantumClassifierFixed(nn.Module):\n",
    "    def __init__(self, n_qubits, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # 학습 가능한 파라미터 (double precision)\n",
    "        self.params = nn.Parameter(torch.randn(n_layers * n_qubits, dtype=torch.float64) * 0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            output = quantum_circuit(x[i], self.params)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        outputs = torch.stack(outputs)\n",
    "        # 시그모이드를 적용하여 확률로 변환 (이미 double precision)\n",
    "        return torch.sigmoid(outputs)\n",
    "\n",
    "# 수정된 모델로 재초기화\n",
    "model = QuantumClassifierFixed(n_qubits, n_layers)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# 훈련 실행\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):\n",
    "    # 훈련\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    # 평가\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "    \n",
    "    # 기록\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "    print()\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 시각화\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# 손실 그래프\n",
    "ax1.plot(train_losses, label='Train Loss', color='blue')\n",
    "ax1.plot(test_losses, label='Test Loss', color='red')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Test Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# 정확도 그래프\n",
    "ax2.plot(train_accuracies, label='Train Accuracy', color='blue')\n",
    "ax2.plot(test_accuracies, label='Test Accuracy', color='red')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Test Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 최종 결과 출력\n",
    "print(f\"Final Test Accuracy: {test_accuracies[-1]:.4f}\")\n",
    "print(f\"Best Test Accuracy: {max(test_accuracies):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 요약 및 제약조건 확인\n",
    "print(\"=== 모델 요약 ===\")\n",
    "print(f\"사용된 큐빗 수: {n_qubits}\")\n",
    "print(f\"회로 레이어 수: {n_layers}\")\n",
    "print(f\"총 학습 가능한 파라미터 수: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(f\"예상 회로 깊이: ~{n_layers * 2 + 1}\")\n",
    "\n",
    "print(\"\\n=== 제약조건 확인 ===\")\n",
    "print(f\"큐빗 수 제한 (≤8): {n_qubits <= 8} ✓\" if n_qubits <= 8 else f\"큐빗 수 제한 (≤8): {n_qubits <= 8} ✗\")\n",
    "print(f\"회로 깊이 제한 (≤30): {n_layers * 2 + 1 <= 30} ✓\" if n_layers * 2 + 1 <= 30 else f\"회로 깊이 제한 (≤30): {n_layers * 2 + 1 <= 30} ✗\")\n",
    "print(f\"파라미터 수 제한 (≤60): {sum(p.numel() for p in model.parameters()) <= 60} ✓\" if sum(p.numel() for p in model.parameters()) <= 60 else f\"파라미터 수 제한 (≤60): {sum(p.numel() for p in model.parameters()) <= 60} ✗\")\n",
    "\n",
    "print(\"\\n=== 사용된 기법 ===\")\n",
    "print(\"- 데이터 인코딩: Angle Encoding\")\n",
    "print(\"- 차원 축소: PCA (784 → 8 차원)\")\n",
    "print(\"- 양자 회로: Variational Quantum Circuit\")\n",
    "print(\"- 얽힘 구조: Circular CNOT connectivity\")\n",
    "print(\"- 측정: Pauli-Z expectation value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0616d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
